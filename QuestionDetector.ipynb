{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionDetector.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "18HVJsNC-WFnvVh7TBXCc5QRhEMLOeWwD",
      "authorship_tag": "ABX9TyPkLMAoVo+GCmvcrDlXUFmu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/QuestionDetector/blob/main/QuestionDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfJ2yUQHfKEa"
      },
      "source": [
        "# Compile Training Data\n",
        "Note: Generate the raw data with [this notebook](https://github.com/daveshap/QuestionDetector/blob/main/DownloadGutenbergTop100.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b76g2sbfHqV",
        "outputId": "93bc1f46-dfef-4b42-b19c-9d4f08a3f213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "datafile = '/content/drive/My Drive/Gutenberg/sentence_data.txt'\n",
        "corpusfile = '/content/drive/My Drive/Gutenberg/corpus_data.txt'\n",
        "testfile = '/content/drive/My Drive/Gutenberg/test_data.txt'\n",
        "sample_cnt = 3000\n",
        "\n",
        "questions = list()\n",
        "exclamations = list()\n",
        "other = list()\n",
        "\n",
        "with open(datafile, 'r', encoding='utf-8') as infile:\n",
        "  body = infile.read()\n",
        "sentences = re.split('\\n\\n', body)\n",
        "\n",
        "for i in sentences:\n",
        "  if 'í' in i or 'á' in i:\n",
        "    continue \n",
        "  if '?' in i:\n",
        "    questions.append(i)\n",
        "  elif '!' in i:\n",
        "    exclamations.append(i)\n",
        "  else:\n",
        "    other.append(i)\n",
        "\n",
        "def flatten_sentence(text):\n",
        "  text = text.lower()\n",
        "  fa = re.findall('[\\w\\s]',text)\n",
        "  return ''.join(fa)\n",
        "\n",
        "corpus = ''\n",
        "random.seed()\n",
        "data = random.sample(questions, sample_cnt)\n",
        "for i in data:\n",
        "  corpus += '<|SENTENCE|> %s <|LABEL|> %s <|END|>\\n\\n' % (flatten_sentence(i), 'question')\n",
        "data = random.sample(exclamations, sample_cnt)\n",
        "for i in data:\n",
        "  corpus += '<|SENTENCE|> %s <|LABEL|> %s <|END|>\\n\\n' % (flatten_sentence(i), 'exclamation')\n",
        "data = random.sample(other, sample_cnt)\n",
        "for i in data:\n",
        "  corpus += '<|SENTENCE|> %s <|LABEL|> %s <|END|>\\n\\n' % (flatten_sentence(i), 'other')\n",
        "with open(corpusfile, 'w', encoding='utf-8') as outfile:\n",
        "  outfile.write(corpus)\n",
        "print('Done!', corpusfile)\n",
        "\n",
        "corpus = ''\n",
        "random.seed()\n",
        "data = random.sample(questions, 50)\n",
        "for i in data:\n",
        "  corpus += '<|SENTENCE|> %s <|LABEL|> %s <|END|>\\n\\n' % (flatten_sentence(i), 'question')\n",
        "data = random.sample(exclamations, 50)\n",
        "for i in data:\n",
        "  corpus += '<|SENTENCE|> %s <|LABEL|> %s <|END|>\\n\\n' % (flatten_sentence(i), 'exclamation')\n",
        "data = random.sample(other, 50)\n",
        "for i in data:\n",
        "  corpus += '<|SENTENCE|> %s <|LABEL|> %s <|END|>\\n\\n' % (flatten_sentence(i), 'other')\n",
        "with open(testfile, 'w', encoding='utf-8') as outfile:\n",
        "  outfile.write(corpus)\n",
        "print('Done!', testfile)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done! /content/drive/My Drive/Gutenberg/corpus_data.txt\n",
            "Done! /content/drive/My Drive/Gutenberg/test_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIXmjYAhB07"
      },
      "source": [
        "# Finetune Model\n",
        "Finetune GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrhH2Eoxk8ra",
        "outputId": "b30cdf73-2bab-4945-ffa3-9515a482f008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 --quiet\n",
        "!pip install gpt-2-simple --quiet\n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# note: manually mount your google drive in the file explorer to the left\n",
        "\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '124M'\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name, model_dir=model_dir)\n",
        "print('\\n\\nModel is ready!')\n",
        "\n",
        "run_name = 'QuestionDetector'\n",
        "step_cnt = 2000\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=corpusfile,\n",
        "              model_name=model_name,\n",
        "              model_dir=model_dir,\n",
        "              checkpoint_dir=checkpoint_dir,\n",
        "              steps=step_cnt,\n",
        "              restore_from='fresh',  # start from scratch\n",
        "              #restore_from='latest',  # continue from last work\n",
        "              run_name=run_name,\n",
        "              print_every=50,\n",
        "              sample_every=1000,\n",
        "              save_every=1000\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 502Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 96.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 322Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:03, 163Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 237Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 172Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 161Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model is ready!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint /content/drive/My Drive/GPT2/models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/GPT2/models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 440110 tokens\n",
            "Training...\n",
            "[50 | 48.70] loss=2.31 avg=2.31\n",
            "[100 | 89.53] loss=2.58 avg=2.44\n",
            "[150 | 130.15] loss=2.51 avg=2.47\n",
            "[200 | 170.86] loss=2.31 avg=2.43\n",
            "[250 | 211.55] loss=1.92 avg=2.32\n",
            "[300 | 252.27] loss=1.76 avg=2.23\n",
            "[350 | 293.07] loss=1.80 avg=2.16\n",
            "[400 | 333.74] loss=1.36 avg=2.06\n",
            "[450 | 374.43] loss=1.47 avg=1.99\n",
            "[500 | 414.98] loss=1.43 avg=1.93\n",
            "[550 | 455.25] loss=1.06 avg=1.85\n",
            "[600 | 495.61] loss=1.00 avg=1.78\n",
            "[650 | 536.04] loss=1.06 avg=1.72\n",
            "[700 | 576.33] loss=0.98 avg=1.66\n",
            "[750 | 616.35] loss=0.83 avg=1.60\n",
            "[800 | 656.75] loss=0.71 avg=1.54\n",
            "[850 | 697.28] loss=0.45 avg=1.47\n",
            "[900 | 737.44] loss=0.53 avg=1.41\n",
            "[950 | 777.86] loss=0.53 avg=1.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKlcLStVlcvQ"
      },
      "source": [
        "# Test Results\n",
        "\n",
        "| Run | Model | Steps | Samples | Last Loss | Avg Loss | Accuracy |\n",
        "|---|---|---|---|---|---|---|\n",
        "| 01 | 124M | 2000 | 9000 |  |  |  |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywrjxls9leZo"
      },
      "source": [
        "right = 0\n",
        "wrong = 0\n",
        "\n",
        "print('Loading test set...')\n",
        "with open(testfile, 'r', encoding='utf-8') as file:\n",
        "  test_set = file.readlines()\n",
        "\n",
        "for t in test_set:\n",
        "  t = t.strip()\n",
        "  if t == '':\n",
        "    continue\n",
        "  prompt = t.split('<|LABEL|>')[0] + '<|LABEL|>'\n",
        "  expect = t.split('<|LABEL|>')[1].replace('<|END|>', '').strip()\n",
        "  #print('\\nPROMPT:', prompt)\n",
        "  response = gpt2.generate(sess, \n",
        "                           return_as_list=True,\n",
        "                           length=30,  # prevent it from going too crazy\n",
        "                           prefix=prompt,\n",
        "                           model_name=model_name,\n",
        "                           model_dir=model_dir,\n",
        "                           truncate='\\n',  # stop inferring here\n",
        "                           include_prefix=False,\n",
        "                           checkpoint_dir=checkpoint_dir,)[0]\n",
        "  response = response.strip()\n",
        "  if expect in response:\n",
        "    right += 1\n",
        "  else:\n",
        "    wrong += 1\n",
        "  print('right:', right, '\\twrong:', wrong, '\\taccuracy:', right / (right+wrong))\n",
        "  #print('RESPONSE:', response)\n",
        "\n",
        "print('\\n\\nModel:', model_name)\n",
        "print('Samples:', max_samples)\n",
        "print('Steps:', step_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
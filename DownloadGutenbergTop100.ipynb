{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DownloadGutenbergTop100.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1kuLrr09pfGYTITDdK-sMmhvD7zhNvYzg",
      "authorship_tag": "ABX9TyNxZYukMjSU7/x4prd2w9aF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/QuestionDetector/blob/main/DownloadGutenbergTop100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhOPluVvdfUI"
      },
      "source": [
        "# Download Top 100 Ebooks\n",
        "Gutenberg has a bunch of books for free. Fiction has a lot of dialog. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcn7U4JkX4uq"
      },
      "source": [
        "import requests\n",
        "\n",
        "prefix = 'https://www.gutenberg.org/files/'\n",
        "# final address example: https://www.gutenberg.org/files/84/84-0.txt\n",
        "books = ['84','1342','25344','2542','11','1952','16328','1250','41','1080','46','5200','98','43','1232','1661','2701','345','844','23','160','209','205','76','1064','42108','408','74','1260','174','2591','16','1497','1400','2852','120','219','215','42884','3207','6130','42324','38269','158','203','2814','514','2500','3825','2600','40074','55','4300','58585','15399','45','1184','36','5740','1727','140','768','902','2554','996','113','3600','2148','19942','57775','244','27827','135','28054','7370','132','20203','13701','1254','1001','1063','11030','32415','779','3296','4363','34901','22381','12122','236','308','4517','35','376','147','1251','766','16643','1998','730']\n",
        "\n",
        "for book in books:\n",
        "  bookurl = prefix + '%s/%s-0.txt' % (book, book)\n",
        "  #print('Downloading:', bookurl)\n",
        "  response = requests.get(bookurl)\n",
        "  body = response.content.decode('utf-8')\n",
        "  with open('/content/drive/My Drive/Gutenberg/%s.txt' % book, 'w', encoding='utf-8') as file:\n",
        "    file.write(body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmOZJmSXdy7O"
      },
      "source": [
        "# Cleanup Docs\n",
        "- Split books based on sections delimited by vertical whitespace\n",
        "- Remove short sections, sections full of all caps, and sections with too many symbols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z74qYaI_eeTd",
        "outputId": "32f0ea80-4c84-4940-e8ce-2ba1ab57c5ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "gut_dir = '/content/drive/My Drive/Gutenberg/'\n",
        "data = list()\n",
        "\n",
        "def clean_chunk(text):\n",
        "  text = text.strip()  # trim the ends\n",
        "  text = re.sub(r'\\t', ' ', text)  # replace tabs with spaces\n",
        "  text = re.sub(r'\\s+', ' ', text)  # replace any extra whitespace with single\n",
        "  return text\n",
        "  \n",
        "def measure_text(text)  :\n",
        "  # returns count of: chars, word chars, digits, whitespace\n",
        "  if len(text) == 0:\n",
        "    return 0, 0, 0, 0\n",
        "  pct_wordchars = len(re.findall('\\w', text)) / len(text)\n",
        "  pct_digits = len(re.findall('\\d', text)) / len(text)\n",
        "  pct_whitespace = len(re.findall('\\s', text)) / len(text)\n",
        "  pct_caps = len(re.findall('[A-Z]', text)) / len(text)\n",
        "  return pct_wordchars, pct_digits, pct_whitespace, pct_caps\n",
        "\n",
        "def split_vertical(text):\n",
        "  results = list()\n",
        "  chunks = re.split('\\n{2,}', text)\n",
        "  for chunk in chunks:\n",
        "    morechunks = re.split('[\\r\\n]{4,}', chunk)\n",
        "    for more in morechunks:\n",
        "      results.append(more)\n",
        "  return results\n",
        "\n",
        "for file in os.listdir(gut_dir):\n",
        "  if 'data' in file:\n",
        "    continue\n",
        "  with open(gut_dir + file, 'r', encoding='utf-8') as infile:\n",
        "    body = infile.read()\n",
        "  if '<!DOCTYPE html>' in body:\n",
        "    continue\n",
        "  chunks = split_vertical(body)\n",
        "  #print('Reading file:', file, '...found chunks:', len(chunks))\n",
        "  for chunk in chunks:\n",
        "    # TODO check chunk for all caps, too many symbols, quotation marks, etc\n",
        "    if '\"' in chunk or 'â€œ' in chunk:  # if text is dialog, put it in\n",
        "      data.append(clean_chunk(chunk))\n",
        "      continue\n",
        "    if '!' in chunk or '?' in chunk:\n",
        "      data.append(clean_chunk(chunk))\n",
        "      continue\n",
        "    if len(chunk) < 15:  # line is just too short\n",
        "      continue\n",
        "    if '.' not in chunk:  # no punctuation at all\n",
        "      continue \n",
        "    pwc, pd, pws, pc = measure_text(chunk)\n",
        "    if pwc < 0.8:  # if too many symbols\n",
        "      continue\n",
        "    if pc > 0.6:  # if too many caps\n",
        "      continue \n",
        "    data.append(clean_chunk(chunk))\n",
        "\n",
        "with open(gut_dir + 'all_data.txt', 'w', encoding='utf-8') as file:\n",
        "  for i in data:\n",
        "    file.write(i + '\\n\\n')\n",
        "\n",
        "print('All done, data saved!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All done, data saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeW5OpJ2TXQn"
      },
      "source": [
        "# Split Sentences\n",
        "Use SpaCy to split sentences along boundaries, save the result with one sentence per line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YqDhtn4TZTD",
        "outputId": "9155a6f2-b83b-4d5e-8815-0233a2067b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install spacy --quiet\n",
        "!pip install pysbd --quiet\n",
        "\n",
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "import os\n",
        "import re\n",
        "\n",
        "gut_dir = '/content/drive/My Drive/Gutenberg/'\n",
        "data = list()\n",
        "\n",
        "with open(gut_dir + 'all_data.txt', 'r', encoding='utf-8') as infile:\n",
        "  body = infile.read()\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "\n",
        "chunks = re.split('\\n\\n', body)\n",
        "print('Processing chunks:', len(chunks))\n",
        "count = 0\n",
        "total = 1\n",
        "for chunk in chunks:\n",
        "  count += 1\n",
        "  total += 1\n",
        "  if count > 5000:\n",
        "    count = 0\n",
        "    print('Processed:', total)\n",
        "  doc = nlp(chunk)\n",
        "  for sent in list(doc.sents):\n",
        "    data.append(sent)\n",
        "\n",
        "with open(gut_dir + 'sentence_data.txt', 'w', encoding='utf-8') as outfile:\n",
        "  for i in data:\n",
        "    outfile.write(str(i) + '\\n\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing chunks: 113896\n",
            "Processed: 5002\n",
            "Processed: 10003\n",
            "Processed: 15004\n",
            "Processed: 20005\n",
            "Processed: 25006\n",
            "Processed: 30007\n",
            "Processed: 35008\n",
            "Processed: 40009\n",
            "Processed: 45010\n",
            "Processed: 50011\n",
            "Processed: 55012\n",
            "Processed: 60013\n",
            "Processed: 65014\n",
            "Processed: 70015\n",
            "Processed: 75016\n",
            "Processed: 80017\n",
            "Processed: 85018\n",
            "Processed: 90019\n",
            "Processed: 95020\n",
            "Processed: 100021\n",
            "Processed: 105022\n",
            "Processed: 110023\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}